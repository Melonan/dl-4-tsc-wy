{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:/data4/conda_envs/g2/bin:/usr/local/cuda-11.6/bin:/home/beihang/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/usr/bin:/data4/conda_envs/g2/bin:/sbin:/usr/local/cuda-10.1/bin:/home/beihang/anaconda3/bin:/home/beihang/anaconda3/condabin:/bin:/usr/bin:/usr/local/bin:/usr/local/cuda-10.1/bin:/home/beihang/anaconda3/bin:/bin:/usr/bin:/bin:/usr/bin:/usr/local/bin:/usr/local/cuda-10.1/bin:/home/beihang/anaconda3/bin:/bin:/usr/bin:/usr/local/cuda/bin:/snap/bin\n",
      "CUDA_HOME:/usr/local/cuda-11.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# # 修改一个已存在的环境变量\n",
    "# os.environ['PATH'] = '/usr/local/cuda-11.6/bin:' + os.environ['PATH']\n",
    "# os.environ['LIBRARY_PATH'] = '/usr/local/cuda-11.6/lib64:' + os.environ['LIBRARY_PATH']\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.6/lib64:'+ os.environ['LD_LIBRARY_PATH']\n",
    "# 使用设置好的环境变量\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "print(f\"PATH:{os.environ['PATH']}\\nCUDA_HOME:{os.environ['CUDA_HOME']}\")\n",
    "dataset_name = \"trial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 08:53:29.456393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-19 08:53:30.375472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# from aeon.datasets.tsc_data_lists import multivariate_equal_length\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time \n",
    "from utils import geng\n",
    "from utils.utils import create_directory\n",
    "from utils import logconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate_equal_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aeon.datasets import load_classification\n",
    "# import numpy as np\n",
    "# dataset_name = \"Libras\"\n",
    "# archive_name=\"MTS\"\n",
    "# X, Y,meta_data = load_classification(dataset_name,return_metadata=True)\n",
    "# X_reshaped = np.transpose(X, (0, 2, 1))\n",
    "# X = X_reshaped\n",
    "# nb_classes = len(np.unique(Y, axis=0))\n",
    "# nb_classes,X.shape,Y.shape,np.unique(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Kailuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140088, 6), (39711, 8), (170, 35))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "archive_name=\"MTS\"\n",
    "time_series1 = pd.read_csv(\"./combined_sheets.csv\")\n",
    "time_series2 = pd.read_csv(\"./combined_sheets2.csv\")\n",
    "meta_data = pd.read_csv(\"./combinde170_info.csv\")\n",
    "time_series1.shape,time_series2.shape,meta_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Sleepstate</th>\n",
       "      <th>Breath</th>\n",
       "      <th>Heartrate</th>\n",
       "      <th>Movement</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-02 16:35:25</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-02 16:36:25</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-07-02 16:37:25</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-02 16:38:25</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07-02 16:39:25</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39706</th>\n",
       "      <td>2023-12-22 22:58:00</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39707</th>\n",
       "      <td>2023-12-22 22:59:00</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39708</th>\n",
       "      <td>2023-12-22 23:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39709</th>\n",
       "      <td>2023-12-22 23:01:00</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39710</th>\n",
       "      <td>2023-12-22 23:02:00</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179799 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Timestamp  Sleepstate  Breath  Heartrate  Movement  Source\n",
       "0      2021-07-02 16:35:25           4      16         78         2       1\n",
       "1      2021-07-02 16:36:25           4      16         76         0       1\n",
       "2      2021-07-02 16:37:25           4      16         74         0       1\n",
       "3      2021-07-02 16:38:25           4      16         74         0       1\n",
       "4      2021-07-02 16:39:25           4      16         73         0       1\n",
       "...                    ...         ...     ...        ...       ...     ...\n",
       "39706  2023-12-22 22:58:00           4      17         86         0     170\n",
       "39707  2023-12-22 22:59:00           2      17         87         0     170\n",
       "39708  2023-12-22 23:00:00           4      17         87         0     170\n",
       "39709  2023-12-22 23:01:00           4      18         88         0     170\n",
       "39710  2023-12-22 23:02:00           4      19         88         0     170\n",
       "\n",
       "[179799 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series2 = time_series2.drop(['SystolicPressure','DiastolicPressure'],axis=1)\n",
    "time_series = pd.concat([time_series1, time_series2], axis=0)\n",
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movement全变成1 二值化\n",
    "# time_series['Movement'] = np.where(time_series['Movement'] != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造每个source的二维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = time_series.groupby('Source')\n",
    "\n",
    "# 提取特征列\n",
    "features = [ 'Breath', 'Heartrate']\n",
    "\n",
    "# # 将每个组转换为二维数组并堆叠\n",
    "# three_dim_array = np.array([group[features].values for _, group in grouped])\n",
    "\n",
    "# three_dim_array.shape # 显示三维数组的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movement 处理\n",
    "- Movement 取消： 只有fold3有提升 其他4个fold下降\n",
    "- Movement 全部+1 不行 都有下降\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_groupby = time_series.groupby('Source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看movement的分布\n",
    "# movement_stac=[{}]\n",
    "# for i in range(1,136):\n",
    "#     print(i)\n",
    "#     keys, vals = np.unique(time_groupby.get_group(int(i))[\"Movement\"].values,return_counts=True)\n",
    "#     result_dict = dict(zip(keys, vals))\n",
    "#     print(result_dict)\n",
    "#     movement_stac.append(result_dict)\n",
    "\n",
    "# # movement_stac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充 截断 时间序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充/截断 构造数据集(去掉过少的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial block \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid sources: Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n",
      "       ...\n",
      "       161, 162, 163, 164, 165, 166, 167, 168, 169, 170],\n",
      "      dtype='int64', name='Source', length=164) \n",
      "obsolete sources: Index([21, 26, 74, 75, 107, 130], dtype='int64', name='Source') \n",
      " len of obsoletes: Source\n",
      "21     148\n",
      "26     251\n",
      "74     193\n",
      "75     150\n",
      "107    415\n",
      "130    188\n",
      "dtype: int64\n",
      "\n",
      " average_time_steps:1088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(164, 1088, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新计算每个样例的时间步数（考虑到了数据过滤）\n",
    "filtered_time_steps_per_source = time_series.groupby('Source').size()\n",
    "\n",
    "# 重新筛选出有效的 Source 过滤掉过少的\n",
    "valid_sources = filtered_time_steps_per_source[filtered_time_steps_per_source > 500].index\n",
    "obsolete_sources = filtered_time_steps_per_source[filtered_time_steps_per_source <= 500].index\n",
    "print(\"valid sources:\", valid_sources, \"\\nobsolete sources:\", obsolete_sources, \"\\n len of obsoletes:\",filtered_time_steps_per_source[obsolete_sources])\n",
    "# 重新获取有效的数据\n",
    "valid_filtered_time_series = time_series[time_series['Source'].isin(valid_sources)]\n",
    "\n",
    "# 重新分组\n",
    "valid_filtered_grouped = valid_filtered_time_series.groupby('Source')\n",
    "\n",
    "# 重新计算平均时间步数（此时应该没有 NaN 值）\n",
    "average_time_steps = int(valid_filtered_grouped.size().mean())\n",
    "\n",
    "print(f\"\\n average_time_steps:{average_time_steps}\")\n",
    "\n",
    "# 初始化新的三维数组（考虑到了有效的样例数）\n",
    "valid_sources_count = len(valid_sources)\n",
    "three_dim_array_valid = np.zeros((valid_sources_count, average_time_steps, len(features)))\n",
    "\n",
    "# 填充新的三维数组\n",
    "for i, (source, group) in enumerate(valid_filtered_grouped):\n",
    "    data = group[features].values\n",
    "    current_steps = data.shape[0]\n",
    "    if current_steps < average_time_steps:\n",
    "        fill_values = {\n",
    "            'Breath': group['Breath'].mean(),\n",
    "            'Heartrate': group['Heartrate'].mean(),\n",
    "        }\n",
    "        fill_array = np.array([[fill_values[feature] for feature in features]] * (average_time_steps - current_steps))\n",
    "        full_data = np.vstack(( fill_array,data))\n",
    "    else:\n",
    "        # 调整为截取较后面的元素\n",
    "        full_data = data[current_steps-average_time_steps:, :]\n",
    "    # print(f\"source:{source},current_steps:{current_steps}, full_data shape:{full_data.shape}\")\n",
    "    three_dim_array_valid[i, :, :] = full_data\n",
    "\n",
    "\n",
    "final_data = three_dim_array_valid\n",
    "\n",
    "# 进行数据标准化\n",
    "final_data = geng.standard_scaler_total(final_data)\n",
    "\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签构造\n",
    "- 意识状态\n",
    "- GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n",
       "       ...\n",
       "       161, 162, 163, 164, 165, 166, 167, 168, 169, 170],\n",
       "      dtype='int64', name='Source', length=164)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n",
       "        2, 2, 2, 1, 1, 1, 1, 1, 2, 1]),\n",
       " (164,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = meta_data[\"意识状态\"].values\n",
    "\n",
    "valid_sources = valid_sources-1\n",
    "labels = [labels[i] for i in range(len(labels)) if i in valid_sources]\n",
    "# 3分类变为2分类\n",
    "# labels = [1 if x == 2 else x for x in labels]\n",
    "nb_classes = len(np.unique(labels, axis=0))\n",
    "labels = np.array(labels)\n",
    "labels, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (填充/截断)构造数据集 原本版 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# average_time_steps = int(grouped.apply(lambda x: x.shape[0]).mean())\n",
    "\n",
    "# # 初始化三维数组\n",
    "# features = ['Sleepstate', 'Breath', 'Heartrate', 'Movement']\n",
    "# three_dim_array = np.zeros((135, average_time_steps, len(features)))\n",
    "\n",
    "# # 填充三维数组\n",
    "# for source, group in grouped:\n",
    "#     data = group[features].values\n",
    "#     current_steps = data.shape[0]\n",
    "\n",
    "#     if current_steps < average_time_steps:\n",
    "#         fill_values = {\n",
    "#             'Breath': group['Breath'].mean(),\n",
    "#             'Heartrate': group['Heartrate'].mean(),\n",
    "#             'Sleepstate': group['Sleepstate'].median(),\n",
    "#             'Movement': 0\n",
    "#         }\n",
    "#         fill_array = np.array([[fill_values[feature] for feature in features]] * (average_time_steps - current_steps))\n",
    "#         full_data = np.vstack((data, fill_array))\n",
    "#     else:\n",
    "#         full_data = data[:average_time_steps, :]\n",
    "\n",
    "#     three_dim_array[source-1, :, :] = full_data\n",
    "\n",
    "# three_dim_array.shape  # 显示三维数组的形状\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签构造\n",
    "- 意识状态\n",
    "- GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = meta_data[\"意识状态\"].values\n",
    "# nb_classes = len(np.unique(labels, axis=0))\n",
    "# labels,nb_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 变为独热编码\n",
    "def data_final_process(x,y):\n",
    "    nb_classes = len(np.unique(y, axis=0))\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
    "    enc.fit(y.reshape(-1, 1))\n",
    "    y = enc.transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "    # # save orignal y because later we will use binary\n",
    "    # y_true = np.argmax(y_test, axis=1)\n",
    "    print(f'x.shape: {x.shape}, y.shape: {y.shape}\\n')\n",
    "    if len(x.shape) == 2:  # if univariate\n",
    "        # add a dimension to make it multivariate with one dimension \n",
    "        x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "    input_shape = x.shape[1:]\n",
    "    print(f'input_shape: {input_shape}\\n')\n",
    "    \n",
    "    return x,y,input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data4/gsprivate/dl-4-tsc/results/resnet/MTS/1-/ exist\n",
      "Creating directory:/data4/gsprivate/dl-4-tsc/results/resnet/MTS/1-/ None\n",
      "x.shape: (164, 1088, 2), y.shape: (164, 3)\n",
      "\n",
      "input_shape: (1088, 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 08:53:34.132195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory:  -> device: 0, name: NVIDIA TITAN X (Pascal), pci bus id: 0000:08:00.0, compute capability: 6.1\n",
      "2024-01-19 08:53:34.133371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 11421 MB memory:  -> device: 1, name: NVIDIA TITAN X (Pascal), pci bus id: 0000:09:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1088, 2)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 1088, 64)             1088      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1088, 64)             256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 1088, 64)             0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 1088, 64)             20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1088, 64)             256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 1088, 64)             0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 1088, 64)             192       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 1088, 64)             12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 1088, 64)             256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 1088, 64)             256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1088, 64)             0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 1088, 64)             0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 1088, 128)            65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1088, 128)            512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 1088, 128)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 1088, 128)            82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1088, 128)            512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 1088, 128)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 1088, 128)            8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 1088, 128)            49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 1088, 128)            512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 1088, 128)            512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1088, 128)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 1088, 128)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 1088, 128)            131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 1088, 128)            512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 1088, 128)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 1088, 128)            82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 1088, 128)            512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 1088, 128)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 1088, 128)            49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 1088, 128)            512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1088, 128)            512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1088, 128)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 1088, 128)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 3)                    387       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 507523 (1.94 MB)\n",
      "Trainable params: 504963 (1.93 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Classifier x.shape=(164, 1088, 2) ,y.shape=(164, 3), input_shape=(1088, 2), nb_classes=3\n"
     ]
    }
   ],
   "source": [
    "classifier_name=\"resnet\"\n",
    "root_dir=\"/data4/gsprivate/dl-4-tsc\"\n",
    "output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '' + '/' + \\\n",
    "                    dataset_name + '/'\n",
    "                    \n",
    "if create_directory(output_directory) is None:\n",
    "    print(\"Creating directory:{} None\".format(output_directory))\n",
    "\n",
    "x,y,input_shape=data_final_process(final_data,labels)\n",
    "resnet_classifier = geng.create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
    "print(f\"Classifier x.shape={x.shape} ,y.shape={y.shape}, input_shape={input_shape}, nb_classes={nb_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 08:53:34 INFO [root] [fit_splits:50] - fold 1: x_train_fold.shape=(131, 1088, 2), y_train_fold.shape=(131, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data4/gsprivate/dl-4-tsc/results/resnet/MTS/1-/fold_1/ exist\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 1088, 2)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 1088, 64)             1088      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 1088, 64)             256       ['conv1d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 1088, 64)             0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 1088, 64)             20544     ['activation_9[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 1088, 64)             256       ['conv1d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 1088, 64)             0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 1088, 64)             192       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 1088, 64)             12352     ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 1088, 64)             256       ['conv1d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 1088, 64)             256       ['conv1d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 1088, 64)             0         ['batch_normalization_15[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 1088, 64)             0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 1088, 128)            65664     ['activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 1088, 128)            512       ['conv1d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 1088, 128)            0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 1088, 128)            82048     ['activation_12[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 1088, 128)            512       ['conv1d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 1088, 128)            0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)          (None, 1088, 128)            8320      ['activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 1088, 128)            49280     ['activation_13[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 1088, 128)            512       ['conv1d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 1088, 128)            512       ['conv1d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 1088, 128)            0         ['batch_normalization_19[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 1088, 128)            0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)          (None, 1088, 128)            131200    ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 1088, 128)            512       ['conv1d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 1088, 128)            0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)          (None, 1088, 128)            82048     ['activation_15[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 1088, 128)            512       ['conv1d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 1088, 128)            0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)          (None, 1088, 128)            49280     ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 1088, 128)            512       ['activation_14[0][0]']       \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 1088, 128)            512       ['conv1d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 1088, 128)            0         ['batch_normalization_23[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, 1088, 128)            0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 128)                  0         ['activation_17[0][0]']       \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 3)                    387       ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 507523 (1.94 MB)\n",
      "Trainable params: 504963 (1.93 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 08:53:35 DEBUG [h5py._conv] [__getitem__:78] - Creating converter from 3 to 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 08:53:40.959673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-01-19 08:53:41.810523: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24574d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-19 08:53:41.810599: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA TITAN X (Pascal), Compute Capability 6.1\n",
      "2024-01-19 08:53:41.810630: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA TITAN X (Pascal), Compute Capability 6.1\n",
      "2024-01-19 08:53:41.821271: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-19 08:53:42.035503: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 13s 136ms/step - loss: 1.1631 - accuracy: 0.5267 - val_loss: 1.1285 - val_accuracy: 0.2121 - lr: 0.0010\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data4/conda_envs/g2/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 53ms/step - loss: 1.1217 - accuracy: 0.4733 - val_loss: 1.1793 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.9329 - accuracy: 0.5267 - val_loss: 1.2221 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.8918 - accuracy: 0.5420 - val_loss: 1.0551 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.9967 - accuracy: 0.5038 - val_loss: 1.0501 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.8713 - accuracy: 0.5878 - val_loss: 1.0870 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.9377 - accuracy: 0.5420 - val_loss: 1.0584 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.8478 - accuracy: 0.5954 - val_loss: 1.0296 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.9010 - accuracy: 0.5496 - val_loss: 1.0609 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8494 - accuracy: 0.5725 - val_loss: 1.0228 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.8267 - accuracy: 0.6489 - val_loss: 0.9988 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8478 - accuracy: 0.5878 - val_loss: 1.0488 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.9164 - accuracy: 0.5802 - val_loss: 1.0489 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8528 - accuracy: 0.5954 - val_loss: 1.0529 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8503 - accuracy: 0.5878 - val_loss: 1.0536 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8280 - accuracy: 0.6183 - val_loss: 1.0537 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8530 - accuracy: 0.5878 - val_loss: 1.0723 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.9138 - accuracy: 0.5420 - val_loss: 1.1763 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8404 - accuracy: 0.5878 - val_loss: 1.1237 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8621 - accuracy: 0.5878 - val_loss: 1.1399 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8477 - accuracy: 0.5802 - val_loss: 1.1548 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8635 - accuracy: 0.6183 - val_loss: 1.1117 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8666 - accuracy: 0.5725 - val_loss: 1.0754 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8395 - accuracy: 0.5954 - val_loss: 1.0583 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8538 - accuracy: 0.5802 - val_loss: 1.0707 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8525 - accuracy: 0.5954 - val_loss: 1.1636 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8723 - accuracy: 0.5725 - val_loss: 1.1544 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8288 - accuracy: 0.5878 - val_loss: 1.1184 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.8256 - accuracy: 0.5802 - val_loss: 1.1643 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.8043 - accuracy: 0.5878 - val_loss: 1.2315 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8104 - accuracy: 0.5878 - val_loss: 1.2549 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8692 - accuracy: 0.5878 - val_loss: 1.3249 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.7936 - accuracy: 0.6260 - val_loss: 1.2495 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8073 - accuracy: 0.5954 - val_loss: 1.2536 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.7887 - accuracy: 0.6260 - val_loss: 1.2834 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8082 - accuracy: 0.5878 - val_loss: 1.2975 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7910 - accuracy: 0.5802 - val_loss: 1.2133 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.7675 - accuracy: 0.6336 - val_loss: 1.2950 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7855 - accuracy: 0.6031 - val_loss: 1.3089 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8032 - accuracy: 0.6260 - val_loss: 1.3425 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7774 - accuracy: 0.6489 - val_loss: 1.4339 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8320 - accuracy: 0.6489 - val_loss: 1.2241 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.7914 - accuracy: 0.6107 - val_loss: 1.1892 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7853 - accuracy: 0.6489 - val_loss: 1.1327 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7661 - accuracy: 0.5954 - val_loss: 1.2439 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.7636 - accuracy: 0.6489 - val_loss: 1.1169 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7615 - accuracy: 0.6107 - val_loss: 1.2659 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7990 - accuracy: 0.6031 - val_loss: 1.1447 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.7557 - accuracy: 0.6412 - val_loss: 1.1694 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.7339 - accuracy: 0.6794 - val_loss: 1.2731 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7550 - accuracy: 0.6031 - val_loss: 1.1780 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7740 - accuracy: 0.6107 - val_loss: 1.2339 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.7021 - accuracy: 0.6870 - val_loss: 1.2348 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7562 - accuracy: 0.6565 - val_loss: 1.4908 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7402 - accuracy: 0.6183 - val_loss: 1.2522 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7823 - accuracy: 0.5954 - val_loss: 1.5470 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7982 - accuracy: 0.6336 - val_loss: 1.3229 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7197 - accuracy: 0.6565 - val_loss: 1.3653 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7184 - accuracy: 0.6718 - val_loss: 1.3287 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.6753 - accuracy: 0.6947 - val_loss: 1.2665 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7236 - accuracy: 0.6718 - val_loss: 1.2472 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7739 - accuracy: 0.6183 - val_loss: 1.2370 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7113 - accuracy: 0.6794 - val_loss: 1.1890 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7817 - accuracy: 0.6107 - val_loss: 1.0789 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7474 - accuracy: 0.6794 - val_loss: 1.3487 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.7176 - accuracy: 0.6489 - val_loss: 2.4363 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7638 - accuracy: 0.6107 - val_loss: 1.5851 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7323 - accuracy: 0.6794 - val_loss: 1.9893 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7781 - accuracy: 0.6641 - val_loss: 1.3350 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6903 - accuracy: 0.6794 - val_loss: 1.7324 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.6385 - accuracy: 0.7481 - val_loss: 1.4434 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6970 - accuracy: 0.6947 - val_loss: 1.4874 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7068 - accuracy: 0.6718 - val_loss: 1.3931 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6436 - accuracy: 0.7023 - val_loss: 1.8265 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.6524 - accuracy: 0.7252 - val_loss: 1.4899 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6601 - accuracy: 0.6947 - val_loss: 1.4947 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6942 - accuracy: 0.6565 - val_loss: 1.2579 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7695 - accuracy: 0.6107 - val_loss: 1.9164 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6671 - accuracy: 0.6947 - val_loss: 1.7263 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6773 - accuracy: 0.6870 - val_loss: 1.9234 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.6045 - accuracy: 0.7176 - val_loss: 2.2737 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5988 - accuracy: 0.7252 - val_loss: 1.5058 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6118 - accuracy: 0.7099 - val_loss: 1.5070 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6671 - accuracy: 0.7099 - val_loss: 1.2935 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6913 - accuracy: 0.7099 - val_loss: 1.3702 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6316 - accuracy: 0.7328 - val_loss: 1.4442 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6293 - accuracy: 0.7023 - val_loss: 1.3355 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5940 - accuracy: 0.7099 - val_loss: 1.6467 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6249 - accuracy: 0.7252 - val_loss: 1.3339 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6068 - accuracy: 0.7634 - val_loss: 1.5797 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6046 - accuracy: 0.7481 - val_loss: 1.5312 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6062 - accuracy: 0.7252 - val_loss: 1.3924 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.5499 - accuracy: 0.7863 - val_loss: 1.6933 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6315 - accuracy: 0.6947 - val_loss: 1.6107 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6570 - accuracy: 0.6794 - val_loss: 1.8112 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5971 - accuracy: 0.7023 - val_loss: 2.8693 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6537 - accuracy: 0.7557 - val_loss: 1.3804 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6911 - accuracy: 0.7328 - val_loss: 1.5181 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6414 - accuracy: 0.7328 - val_loss: 1.6685 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5629 - accuracy: 0.7252 - val_loss: 1.4198 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5664 - accuracy: 0.7176 - val_loss: 1.6413 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6097 - accuracy: 0.7252 - val_loss: 1.6404 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5851 - accuracy: 0.7939 - val_loss: 1.7695 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6205 - accuracy: 0.7328 - val_loss: 1.6051 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6049 - accuracy: 0.7634 - val_loss: 2.3809 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6580 - accuracy: 0.7328 - val_loss: 1.4875 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5940 - accuracy: 0.7481 - val_loss: 1.9883 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5681 - accuracy: 0.7328 - val_loss: 1.9984 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5917 - accuracy: 0.7023 - val_loss: 2.2410 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5802 - accuracy: 0.7634 - val_loss: 2.1402 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5580 - accuracy: 0.7099 - val_loss: 1.5268 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5553 - accuracy: 0.7328 - val_loss: 1.6629 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5139 - accuracy: 0.8168 - val_loss: 2.1247 - val_accuracy: 0.1818 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.4847 - accuracy: 0.7710 - val_loss: 1.9249 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6106 - accuracy: 0.7252 - val_loss: 2.3325 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5473 - accuracy: 0.7786 - val_loss: 1.6690 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.4498 - accuracy: 0.8168 - val_loss: 1.6661 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5051 - accuracy: 0.8015 - val_loss: 1.5971 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.4244 - accuracy: 0.8397 - val_loss: 1.6456 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.4034 - accuracy: 0.8473 - val_loss: 1.8162 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.3646 - accuracy: 0.9008 - val_loss: 1.9326 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4521 - accuracy: 0.8015 - val_loss: 1.7812 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4972 - accuracy: 0.7786 - val_loss: 1.7534 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4825 - accuracy: 0.8244 - val_loss: 1.8077 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5293 - accuracy: 0.7939 - val_loss: 1.9546 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4784 - accuracy: 0.7863 - val_loss: 2.2518 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5304 - accuracy: 0.7252 - val_loss: 2.0112 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5664 - accuracy: 0.7557 - val_loss: 2.2348 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6207 - accuracy: 0.7786 - val_loss: 2.0433 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5604 - accuracy: 0.7786 - val_loss: 2.4760 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5355 - accuracy: 0.7405 - val_loss: 2.4292 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5002 - accuracy: 0.7786 - val_loss: 1.9308 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.3599 - accuracy: 0.9160 - val_loss: 1.8389 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3843 - accuracy: 0.8779 - val_loss: 1.7041 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.3293 - accuracy: 0.8931 - val_loss: 1.9534 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3989 - accuracy: 0.8397 - val_loss: 2.0849 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3690 - accuracy: 0.8855 - val_loss: 2.1410 - val_accuracy: 0.2121 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3867 - accuracy: 0.8702 - val_loss: 2.3634 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3453 - accuracy: 0.8855 - val_loss: 1.9465 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4101 - accuracy: 0.8397 - val_loss: 2.7148 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.4181 - accuracy: 0.8397 - val_loss: 1.8952 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4194 - accuracy: 0.8779 - val_loss: 1.8304 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.4422 - accuracy: 0.8321 - val_loss: 1.4721 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3728 - accuracy: 0.8626 - val_loss: 2.1915 - val_accuracy: 0.1818 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3472 - accuracy: 0.8626 - val_loss: 1.6170 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3774 - accuracy: 0.8855 - val_loss: 1.7674 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.4093 - accuracy: 0.8473 - val_loss: 1.8867 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.3091 - accuracy: 0.9084 - val_loss: 1.8165 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.2905 - accuracy: 0.9008 - val_loss: 1.7043 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.2533 - accuracy: 0.9237 - val_loss: 1.4051 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2659 - accuracy: 0.9008 - val_loss: 1.4966 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3506 - accuracy: 0.8779 - val_loss: 1.5911 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3114 - accuracy: 0.8931 - val_loss: 2.1015 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4014 - accuracy: 0.8473 - val_loss: 1.7723 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3616 - accuracy: 0.8321 - val_loss: 1.9316 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3020 - accuracy: 0.8779 - val_loss: 1.9128 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3400 - accuracy: 0.9008 - val_loss: 2.0236 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3200 - accuracy: 0.8931 - val_loss: 2.1226 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3415 - accuracy: 0.8550 - val_loss: 1.7194 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3668 - accuracy: 0.8626 - val_loss: 1.5514 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3445 - accuracy: 0.8702 - val_loss: 2.0957 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3461 - accuracy: 0.8550 - val_loss: 2.2566 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3035 - accuracy: 0.9389 - val_loss: 2.4571 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3080 - accuracy: 0.8702 - val_loss: 1.7670 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2817 - accuracy: 0.9008 - val_loss: 1.5956 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.2233 - accuracy: 0.9542 - val_loss: 1.7463 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2538 - accuracy: 0.9389 - val_loss: 2.4634 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.2092 - accuracy: 0.9542 - val_loss: 2.5541 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2501 - accuracy: 0.9160 - val_loss: 2.7778 - val_accuracy: 0.0909 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2132 - accuracy: 0.9542 - val_loss: 1.9473 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2720 - accuracy: 0.9008 - val_loss: 2.4266 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3110 - accuracy: 0.8931 - val_loss: 3.8096 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2882 - accuracy: 0.9237 - val_loss: 2.0555 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3341 - accuracy: 0.9008 - val_loss: 2.3842 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2691 - accuracy: 0.9008 - val_loss: 2.9869 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3992 - accuracy: 0.8244 - val_loss: 2.7332 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.3349 - accuracy: 0.8931 - val_loss: 2.2699 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4177 - accuracy: 0.8626 - val_loss: 4.1700 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3756 - accuracy: 0.8626 - val_loss: 2.9719 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2875 - accuracy: 0.8931 - val_loss: 2.5376 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2836 - accuracy: 0.9237 - val_loss: 2.1854 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.2212 - accuracy: 0.9237 - val_loss: 1.8940 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.2089 - accuracy: 0.9313 - val_loss: 2.1136 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.1643 - accuracy: 0.9542 - val_loss: 2.1593 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.1475 - accuracy: 0.9771 - val_loss: 1.9222 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.1317 - accuracy: 0.9924 - val_loss: 1.7452 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1763 - accuracy: 0.9389 - val_loss: 1.8973 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2228 - accuracy: 0.9160 - val_loss: 2.4512 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2649 - accuracy: 0.9084 - val_loss: 2.3695 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.1768 - accuracy: 0.9466 - val_loss: 2.3198 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.1845 - accuracy: 0.9466 - val_loss: 1.9932 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2442 - accuracy: 0.9389 - val_loss: 2.3206 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2140 - accuracy: 0.9389 - val_loss: 2.2282 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1931 - accuracy: 0.9313 - val_loss: 1.9657 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2146 - accuracy: 0.9313 - val_loss: 2.1198 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1582 - accuracy: 0.9695 - val_loss: 2.3751 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.1146 - accuracy: 0.9771 - val_loss: 2.5124 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.0955 - accuracy: 0.9847 - val_loss: 2.0256 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1487 - accuracy: 0.9542 - val_loss: 1.8158 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.1185 - accuracy: 0.9771 - val_loss: 2.2530 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "2/2 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 08:55:14 DEBUG [matplotlib.pyplot] [switch_backend:339] - Loaded backend agg version v2.2.\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1379] - findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0.\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', name='Liberation Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf', name='Liberation Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf', name='Liberation Sans Narrow', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', name='Liberation Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf', name='Liberation Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', name='Liberation Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf', name='Liberation Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf', name='Liberation Sans Narrow', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1422] - findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to Liberation Sans ('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf') with score of 0.050000.\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1379] - findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0.\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/data4/conda_envs/g2/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', name='Liberation Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf', name='Liberation Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf', name='Liberation Sans Narrow', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', name='Liberation Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf', name='Liberation Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', name='Liberation Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf', name='Liberation Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1391] - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf', name='Liberation Sans Narrow', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535\n",
      "2024-01-19 08:55:14 DEBUG [matplotlib.font_manager] [_findfont_cached:1422] - findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to Liberation Sans ('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf') with score of 0.050000.\n",
      "2024-01-19 08:55:14 INFO [root] [fit_splits:50] - fold 2: x_train_fold.shape=(131, 1088, 2), y_train_fold.shape=(131, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data4/gsprivate/dl-4-tsc/results/resnet/MTS/1-/fold_2/ exist\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1088, 2)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 1088, 64)             1088      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1088, 64)             256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 1088, 64)             0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 1088, 64)             20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1088, 64)             256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 1088, 64)             0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 1088, 64)             192       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 1088, 64)             12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 1088, 64)             256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 1088, 64)             256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1088, 64)             0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 1088, 64)             0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 1088, 128)            65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1088, 128)            512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 1088, 128)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 1088, 128)            82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1088, 128)            512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 1088, 128)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 1088, 128)            8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 1088, 128)            49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 1088, 128)            512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 1088, 128)            512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1088, 128)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 1088, 128)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 1088, 128)            131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 1088, 128)            512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 1088, 128)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 1088, 128)            82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 1088, 128)            512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 1088, 128)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 1088, 128)            49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 1088, 128)            512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1088, 128)            512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1088, 128)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 1088, 128)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 3)                    387       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 507523 (1.94 MB)\n",
      "Trainable params: 504963 (1.93 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - 11s 112ms/step - loss: 1.2782 - accuracy: 0.4580 - val_loss: 1.3908 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 2/200\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.9566 - accuracy: 0.6154"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data4/conda_envs/g2/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 47ms/step - loss: 0.9476 - accuracy: 0.6336 - val_loss: 1.1864 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.9534 - accuracy: 0.5496 - val_loss: 1.5037 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.8820 - accuracy: 0.6031 - val_loss: 1.3056 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.9253 - accuracy: 0.5573 - val_loss: 1.1875 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.8675 - accuracy: 0.5725 - val_loss: 1.1702 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.9376 - accuracy: 0.5802 - val_loss: 1.0209 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.8612 - accuracy: 0.5802 - val_loss: 1.0428 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.8177 - accuracy: 0.6260 - val_loss: 1.0511 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8198 - accuracy: 0.5802 - val_loss: 1.0444 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.8329 - accuracy: 0.6183 - val_loss: 1.0629 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8219 - accuracy: 0.6260 - val_loss: 1.0439 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8232 - accuracy: 0.6031 - val_loss: 1.0388 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8130 - accuracy: 0.6107 - val_loss: 1.0463 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8525 - accuracy: 0.6031 - val_loss: 1.0837 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8167 - accuracy: 0.6412 - val_loss: 1.1011 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7719 - accuracy: 0.6794 - val_loss: 1.1420 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8919 - accuracy: 0.6031 - val_loss: 1.1529 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8101 - accuracy: 0.6336 - val_loss: 1.2122 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8723 - accuracy: 0.5954 - val_loss: 1.1185 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8359 - accuracy: 0.6107 - val_loss: 1.2035 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.7709 - accuracy: 0.6794 - val_loss: 1.3274 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8408 - accuracy: 0.6107 - val_loss: 1.2711 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7871 - accuracy: 0.6183 - val_loss: 1.2842 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.7975 - accuracy: 0.6260 - val_loss: 1.2993 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7894 - accuracy: 0.5954 - val_loss: 1.3045 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.8203 - accuracy: 0.6183 - val_loss: 1.2165 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7866 - accuracy: 0.6565 - val_loss: 1.2989 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8190 - accuracy: 0.5954 - val_loss: 1.4297 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7851 - accuracy: 0.6489 - val_loss: 1.3436 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.7708 - accuracy: 0.6489 - val_loss: 1.5389 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.7900 - accuracy: 0.6183 - val_loss: 1.4413 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7898 - accuracy: 0.6336 - val_loss: 1.5028 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.7528 - accuracy: 0.6412 - val_loss: 1.4290 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.7505 - accuracy: 0.6718 - val_loss: 1.6664 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7858 - accuracy: 0.6565 - val_loss: 1.5053 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7380 - accuracy: 0.7023 - val_loss: 1.7431 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7516 - accuracy: 0.6870 - val_loss: 1.6849 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7384 - accuracy: 0.6641 - val_loss: 1.5843 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7552 - accuracy: 0.6489 - val_loss: 1.6051 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8191 - accuracy: 0.6260 - val_loss: 1.8466 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7498 - accuracy: 0.6412 - val_loss: 2.1402 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.7369 - accuracy: 0.7176 - val_loss: 1.8694 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7657 - accuracy: 0.6489 - val_loss: 1.8807 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7596 - accuracy: 0.6641 - val_loss: 1.9938 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7802 - accuracy: 0.6260 - val_loss: 2.0108 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7721 - accuracy: 0.6336 - val_loss: 1.5464 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.7112 - accuracy: 0.7099 - val_loss: 2.0895 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7477 - accuracy: 0.6565 - val_loss: 2.1871 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.7296 - accuracy: 0.6947 - val_loss: 2.0766 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7772 - accuracy: 0.5878 - val_loss: 1.7911 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.6383 - accuracy: 0.7328 - val_loss: 1.8558 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.6244 - accuracy: 0.7252 - val_loss: 1.7947 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6728 - accuracy: 0.7252 - val_loss: 1.4020 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6837 - accuracy: 0.7023 - val_loss: 1.8461 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7091 - accuracy: 0.7023 - val_loss: 1.8111 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7231 - accuracy: 0.6641 - val_loss: 1.4916 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7624 - accuracy: 0.6718 - val_loss: 2.2439 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7692 - accuracy: 0.6718 - val_loss: 1.9153 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6584 - accuracy: 0.7099 - val_loss: 2.0963 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7223 - accuracy: 0.7328 - val_loss: 1.6306 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6778 - accuracy: 0.7176 - val_loss: 1.8056 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.6236 - accuracy: 0.7405 - val_loss: 1.8843 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6645 - accuracy: 0.7710 - val_loss: 2.0355 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.6486 - accuracy: 0.7252 - val_loss: 2.1476 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.6157 - accuracy: 0.7328 - val_loss: 2.2148 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6340 - accuracy: 0.7176 - val_loss: 2.0034 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7359 - accuracy: 0.6947 - val_loss: 2.5572 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7100 - accuracy: 0.6794 - val_loss: 2.6207 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.6605 - accuracy: 0.7023 - val_loss: 2.3146 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6737 - accuracy: 0.7023 - val_loss: 1.9390 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6453 - accuracy: 0.7023 - val_loss: 1.9623 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6361 - accuracy: 0.7405 - val_loss: 1.9189 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6443 - accuracy: 0.6870 - val_loss: 2.3917 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6494 - accuracy: 0.7557 - val_loss: 2.1248 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6172 - accuracy: 0.7481 - val_loss: 2.5140 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6681 - accuracy: 0.6794 - val_loss: 2.7403 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.7418 - accuracy: 0.6336 - val_loss: 1.6194 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6302 - accuracy: 0.6947 - val_loss: 2.4810 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.6086 - accuracy: 0.7328 - val_loss: 2.3368 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6141 - accuracy: 0.7786 - val_loss: 2.1756 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6415 - accuracy: 0.6794 - val_loss: 2.1229 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5281 - accuracy: 0.7634 - val_loss: 2.2348 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5904 - accuracy: 0.7481 - val_loss: 2.5303 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6005 - accuracy: 0.7176 - val_loss: 2.1870 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 0.5952 - accuracy: 0.7634 - val_loss: 1.7536 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5830 - accuracy: 0.7863 - val_loss: 1.8345 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 47ms/step - loss: 0.4837 - accuracy: 0.7863 - val_loss: 1.8300 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5496 - accuracy: 0.7939 - val_loss: 2.2910 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5466 - accuracy: 0.7863 - val_loss: 1.9593 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5684 - accuracy: 0.7481 - val_loss: 3.5337 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6008 - accuracy: 0.7710 - val_loss: 3.8621 - val_accuracy: 0.1212 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5299 - accuracy: 0.7786 - val_loss: 2.2037 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.4956 - accuracy: 0.7939 - val_loss: 2.0980 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.4663 - accuracy: 0.8550 - val_loss: 2.4240 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.4341 - accuracy: 0.8168 - val_loss: 2.1297 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.4078 - accuracy: 0.8855 - val_loss: 2.1810 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4791 - accuracy: 0.7863 - val_loss: 2.5140 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5173 - accuracy: 0.7710 - val_loss: 2.3317 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5130 - accuracy: 0.7786 - val_loss: 1.9950 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5251 - accuracy: 0.7786 - val_loss: 2.1497 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4511 - accuracy: 0.8015 - val_loss: 2.0299 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4133 - accuracy: 0.8626 - val_loss: 2.0878 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4571 - accuracy: 0.8092 - val_loss: 2.3790 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5048 - accuracy: 0.7863 - val_loss: 2.3719 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4532 - accuracy: 0.7939 - val_loss: 2.5627 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4253 - accuracy: 0.8626 - val_loss: 2.6346 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.3703 - accuracy: 0.8702 - val_loss: 2.0464 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4136 - accuracy: 0.8702 - val_loss: 2.2589 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.4371 - accuracy: 0.8092 - val_loss: 2.1541 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4419 - accuracy: 0.8244 - val_loss: 2.2093 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5662 - accuracy: 0.7786 - val_loss: 4.0952 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6380 - accuracy: 0.7176 - val_loss: 5.2914 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4758 - accuracy: 0.8244 - val_loss: 4.0542 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5187 - accuracy: 0.8168 - val_loss: 2.6612 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4997 - accuracy: 0.7863 - val_loss: 4.4288 - val_accuracy: 0.1818 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4961 - accuracy: 0.7786 - val_loss: 2.8488 - val_accuracy: 0.1818 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.3632 - accuracy: 0.8626 - val_loss: 2.1154 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4147 - accuracy: 0.7863 - val_loss: 2.0648 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4972 - accuracy: 0.8244 - val_loss: 2.4929 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4514 - accuracy: 0.8321 - val_loss: 3.4238 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4794 - accuracy: 0.8092 - val_loss: 2.3245 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.3563 - accuracy: 0.8855 - val_loss: 2.4857 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3898 - accuracy: 0.8397 - val_loss: 2.7667 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4136 - accuracy: 0.8779 - val_loss: 2.6357 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3884 - accuracy: 0.8626 - val_loss: 2.1620 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4163 - accuracy: 0.8015 - val_loss: 3.0013 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5029 - accuracy: 0.8397 - val_loss: 3.1228 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4112 - accuracy: 0.8321 - val_loss: 2.0807 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3482 - accuracy: 0.9008 - val_loss: 2.6199 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3641 - accuracy: 0.8626 - val_loss: 2.0946 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4336 - accuracy: 0.8244 - val_loss: 1.7006 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.3198 - accuracy: 0.8779 - val_loss: 2.3246 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.2880 - accuracy: 0.9008 - val_loss: 2.1505 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3301 - accuracy: 0.8931 - val_loss: 2.0969 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.2819 - accuracy: 0.8855 - val_loss: 2.1317 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3243 - accuracy: 0.8702 - val_loss: 2.8614 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3472 - accuracy: 0.8550 - val_loss: 2.5893 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3214 - accuracy: 0.8626 - val_loss: 1.7089 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.2621 - accuracy: 0.9160 - val_loss: 2.0513 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.2446 - accuracy: 0.9466 - val_loss: 2.1578 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2847 - accuracy: 0.8931 - val_loss: 2.0523 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3158 - accuracy: 0.8626 - val_loss: 1.8344 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2957 - accuracy: 0.9008 - val_loss: 2.0090 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3393 - accuracy: 0.8550 - val_loss: 2.1329 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3359 - accuracy: 0.9084 - val_loss: 3.5541 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2910 - accuracy: 0.9466 - val_loss: 2.7142 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.2418 - accuracy: 0.9160 - val_loss: 2.6320 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3460 - accuracy: 0.8702 - val_loss: 2.8502 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2622 - accuracy: 0.9008 - val_loss: 3.2166 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2529 - accuracy: 0.9542 - val_loss: 2.9311 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2906 - accuracy: 0.9084 - val_loss: 2.7367 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3440 - accuracy: 0.8931 - val_loss: 2.6937 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3014 - accuracy: 0.9084 - val_loss: 4.1617 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3132 - accuracy: 0.8855 - val_loss: 1.9963 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3037 - accuracy: 0.8702 - val_loss: 2.3875 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3168 - accuracy: 0.8550 - val_loss: 2.2682 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.2408 - accuracy: 0.9237 - val_loss: 2.6075 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2915 - accuracy: 0.9160 - val_loss: 2.6777 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2520 - accuracy: 0.9389 - val_loss: 2.3399 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2639 - accuracy: 0.9160 - val_loss: 3.8998 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2521 - accuracy: 0.9237 - val_loss: 3.2717 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.2250 - accuracy: 0.9313 - val_loss: 2.5208 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2919 - accuracy: 0.8702 - val_loss: 2.2181 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.1916 - accuracy: 0.9389 - val_loss: 2.0994 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.1683 - accuracy: 0.9695 - val_loss: 2.6853 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2180 - accuracy: 0.9084 - val_loss: 2.5523 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2040 - accuracy: 0.9313 - val_loss: 2.1361 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3581 - accuracy: 0.8702 - val_loss: 2.2817 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2448 - accuracy: 0.9313 - val_loss: 2.3132 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3506 - accuracy: 0.8855 - val_loss: 2.2855 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3247 - accuracy: 0.8626 - val_loss: 3.4606 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3258 - accuracy: 0.9008 - val_loss: 3.2960 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2310 - accuracy: 0.9237 - val_loss: 2.2514 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2948 - accuracy: 0.8931 - val_loss: 2.6828 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2612 - accuracy: 0.9313 - val_loss: 2.4944 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2471 - accuracy: 0.9160 - val_loss: 2.3106 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2046 - accuracy: 0.9466 - val_loss: 2.0958 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2152 - accuracy: 0.9389 - val_loss: 2.5841 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2305 - accuracy: 0.9466 - val_loss: 2.3770 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1895 - accuracy: 0.9542 - val_loss: 2.3499 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2151 - accuracy: 0.9389 - val_loss: 2.7537 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2676 - accuracy: 0.8779 - val_loss: 2.6136 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1841 - accuracy: 0.9389 - val_loss: 2.7037 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1720 - accuracy: 0.9466 - val_loss: 3.5609 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 1s 58ms/step - loss: 0.1495 - accuracy: 0.9847 - val_loss: 2.3831 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1626 - accuracy: 0.9389 - val_loss: 2.8565 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.1071 - accuracy: 0.9924 - val_loss: 2.2572 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1327 - accuracy: 0.9618 - val_loss: 2.2187 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1634 - accuracy: 0.9771 - val_loss: 3.9481 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.1899 - accuracy: 0.9389 - val_loss: 4.3947 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.2653 - accuracy: 0.9008 - val_loss: 2.0936 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1801 - accuracy: 0.9466 - val_loss: 2.9670 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2286 - accuracy: 0.9618 - val_loss: 7.2113 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3061 - accuracy: 0.9008 - val_loss: 6.9066 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3340 - accuracy: 0.8931 - val_loss: 2.7421 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2533 - accuracy: 0.9008 - val_loss: 2.4204 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2491 - accuracy: 0.9160 - val_loss: 3.3496 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1664 - accuracy: 0.9542 - val_loss: 2.8075 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2594 - accuracy: 0.9237 - val_loss: 2.9756 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "2/2 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 08:56:50 INFO [root] [fit_splits:50] - fold 3: x_train_fold.shape=(131, 1088, 2), y_train_fold.shape=(131, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data4/gsprivate/dl-4-tsc/results/resnet/MTS/1-/fold_3/ exist\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1088, 2)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 1088, 64)             1088      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1088, 64)             256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 1088, 64)             0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 1088, 64)             20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1088, 64)             256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 1088, 64)             0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 1088, 64)             192       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 1088, 64)             12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 1088, 64)             256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 1088, 64)             256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1088, 64)             0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 1088, 64)             0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 1088, 128)            65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1088, 128)            512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 1088, 128)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 1088, 128)            82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1088, 128)            512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 1088, 128)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 1088, 128)            8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 1088, 128)            49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 1088, 128)            512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 1088, 128)            512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1088, 128)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 1088, 128)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 1088, 128)            131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 1088, 128)            512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 1088, 128)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 1088, 128)            82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 1088, 128)            512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 1088, 128)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 1088, 128)            49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 1088, 128)            512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1088, 128)            512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1088, 128)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 1088, 128)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 3)                    387       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 507523 (1.94 MB)\n",
      "Trainable params: 504963 (1.93 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1932 - accuracy: 0.5038"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data4/conda_envs/g2/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 113ms/step - loss: 1.1932 - accuracy: 0.5038 - val_loss: 1.0939 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 1.0381 - accuracy: 0.5496 - val_loss: 1.1048 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.9322 - accuracy: 0.5725 - val_loss: 1.1115 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.8812 - accuracy: 0.5954 - val_loss: 1.1191 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 0.8421 - accuracy: 0.6336 - val_loss: 1.1157 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8856 - accuracy: 0.5878 - val_loss: 1.1037 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.8375 - accuracy: 0.6031 - val_loss: 1.0507 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.8812 - accuracy: 0.5802 - val_loss: 0.9708 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.9389 - accuracy: 0.5649 - val_loss: 1.0148 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8378 - accuracy: 0.5954 - val_loss: 1.0545 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8596 - accuracy: 0.6183 - val_loss: 1.0421 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8520 - accuracy: 0.6107 - val_loss: 1.0837 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8849 - accuracy: 0.5878 - val_loss: 1.0396 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8326 - accuracy: 0.5954 - val_loss: 1.0727 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8619 - accuracy: 0.6260 - val_loss: 1.1026 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8663 - accuracy: 0.6183 - val_loss: 1.1661 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8704 - accuracy: 0.5954 - val_loss: 1.2118 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8485 - accuracy: 0.6031 - val_loss: 1.0609 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8336 - accuracy: 0.6260 - val_loss: 1.0349 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8396 - accuracy: 0.6489 - val_loss: 1.0036 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8384 - accuracy: 0.6260 - val_loss: 0.9625 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8363 - accuracy: 0.6260 - val_loss: 1.0849 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8240 - accuracy: 0.6260 - val_loss: 1.0691 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8136 - accuracy: 0.6794 - val_loss: 1.2087 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8297 - accuracy: 0.6107 - val_loss: 1.1882 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8391 - accuracy: 0.6489 - val_loss: 1.4790 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8420 - accuracy: 0.5954 - val_loss: 1.1819 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.7932 - accuracy: 0.6412 - val_loss: 1.2293 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.7749 - accuracy: 0.6565 - val_loss: 1.1919 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7930 - accuracy: 0.6183 - val_loss: 1.1734 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8261 - accuracy: 0.5954 - val_loss: 1.0556 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8215 - accuracy: 0.6336 - val_loss: 0.9355 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8433 - accuracy: 0.5878 - val_loss: 1.0238 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.7949 - accuracy: 0.6260 - val_loss: 1.1540 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8376 - accuracy: 0.6412 - val_loss: 1.1789 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7824 - accuracy: 0.6336 - val_loss: 1.0523 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.7275 - accuracy: 0.6183 - val_loss: 1.0664 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7775 - accuracy: 0.6412 - val_loss: 0.9815 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.7153 - accuracy: 0.6794 - val_loss: 1.3878 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.7659 - accuracy: 0.6641 - val_loss: 1.2000 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7678 - accuracy: 0.6565 - val_loss: 1.1901 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.8357 - accuracy: 0.6412 - val_loss: 1.1212 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7975 - accuracy: 0.6183 - val_loss: 1.0597 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7998 - accuracy: 0.6565 - val_loss: 1.0995 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7416 - accuracy: 0.6718 - val_loss: 1.0637 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8018 - accuracy: 0.6565 - val_loss: 1.3331 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7729 - accuracy: 0.6412 - val_loss: 1.4909 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7514 - accuracy: 0.6489 - val_loss: 1.3036 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7690 - accuracy: 0.6641 - val_loss: 1.1041 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7444 - accuracy: 0.6641 - val_loss: 1.8869 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.7051 - accuracy: 0.6870 - val_loss: 1.7393 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7250 - accuracy: 0.6870 - val_loss: 1.3565 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7138 - accuracy: 0.7023 - val_loss: 1.7646 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7641 - accuracy: 0.6718 - val_loss: 1.3344 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7704 - accuracy: 0.6183 - val_loss: 1.3474 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7675 - accuracy: 0.6336 - val_loss: 1.3104 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7403 - accuracy: 0.6412 - val_loss: 1.1332 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.6763 - accuracy: 0.6870 - val_loss: 1.2859 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7246 - accuracy: 0.6489 - val_loss: 1.0979 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7285 - accuracy: 0.6870 - val_loss: 1.2120 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7258 - accuracy: 0.6718 - val_loss: 1.1881 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6876 - accuracy: 0.6718 - val_loss: 1.2455 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.6738 - accuracy: 0.7328 - val_loss: 1.4226 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.6682 - accuracy: 0.6947 - val_loss: 1.4262 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7481 - accuracy: 0.6412 - val_loss: 1.1989 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.6271 - accuracy: 0.7710 - val_loss: 1.6855 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6589 - accuracy: 0.7099 - val_loss: 1.4245 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6510 - accuracy: 0.7023 - val_loss: 1.7333 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6715 - accuracy: 0.7023 - val_loss: 1.5032 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7151 - accuracy: 0.6947 - val_loss: 1.1342 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7111 - accuracy: 0.7405 - val_loss: 2.4862 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6548 - accuracy: 0.6947 - val_loss: 1.4656 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7524 - accuracy: 0.6794 - val_loss: 1.7390 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6588 - accuracy: 0.7176 - val_loss: 1.2732 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6770 - accuracy: 0.7023 - val_loss: 1.2110 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.6071 - accuracy: 0.7405 - val_loss: 1.6120 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6796 - accuracy: 0.7405 - val_loss: 1.3457 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6264 - accuracy: 0.6947 - val_loss: 1.0308 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7464 - accuracy: 0.6718 - val_loss: 1.2901 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6362 - accuracy: 0.7176 - val_loss: 1.6796 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6298 - accuracy: 0.7481 - val_loss: 1.6496 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6413 - accuracy: 0.7481 - val_loss: 1.7094 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6162 - accuracy: 0.7099 - val_loss: 1.4369 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7362 - accuracy: 0.6489 - val_loss: 1.3163 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6511 - accuracy: 0.7176 - val_loss: 1.4472 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.5535 - accuracy: 0.7710 - val_loss: 1.4470 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6011 - accuracy: 0.7252 - val_loss: 1.2926 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6160 - accuracy: 0.7328 - val_loss: 1.6782 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6475 - accuracy: 0.7634 - val_loss: 1.9795 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5857 - accuracy: 0.7481 - val_loss: 2.0468 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5910 - accuracy: 0.7710 - val_loss: 1.5713 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5995 - accuracy: 0.7786 - val_loss: 1.4752 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6518 - accuracy: 0.7405 - val_loss: 1.4560 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5206 - accuracy: 0.8092 - val_loss: 1.5988 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6428 - accuracy: 0.7481 - val_loss: 2.0321 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6093 - accuracy: 0.7328 - val_loss: 1.6418 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5811 - accuracy: 0.7176 - val_loss: 1.7877 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5982 - accuracy: 0.7710 - val_loss: 3.3238 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.6234 - accuracy: 0.7405 - val_loss: 1.2825 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5617 - accuracy: 0.8015 - val_loss: 1.4364 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5721 - accuracy: 0.7710 - val_loss: 1.5401 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5463 - accuracy: 0.7786 - val_loss: 1.4018 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.5189 - accuracy: 0.7557 - val_loss: 1.6808 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.4955 - accuracy: 0.7863 - val_loss: 1.8016 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5450 - accuracy: 0.7710 - val_loss: 2.1257 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6093 - accuracy: 0.7405 - val_loss: 1.2240 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5493 - accuracy: 0.8015 - val_loss: 1.9482 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5308 - accuracy: 0.7939 - val_loss: 1.4337 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 1s 50ms/step - loss: 0.4291 - accuracy: 0.8244 - val_loss: 1.8361 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4294 - accuracy: 0.8397 - val_loss: 1.6213 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.4922 - accuracy: 0.8397 - val_loss: 2.3943 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5543 - accuracy: 0.7557 - val_loss: 1.9445 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5018 - accuracy: 0.7557 - val_loss: 1.6938 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4914 - accuracy: 0.8244 - val_loss: 2.3776 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5774 - accuracy: 0.7710 - val_loss: 2.0533 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5652 - accuracy: 0.7634 - val_loss: 1.5843 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6352 - accuracy: 0.7252 - val_loss: 2.8915 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5127 - accuracy: 0.7939 - val_loss: 3.3934 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5046 - accuracy: 0.8092 - val_loss: 1.9750 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5048 - accuracy: 0.7939 - val_loss: 1.6080 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4386 - accuracy: 0.8168 - val_loss: 1.8357 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4328 - accuracy: 0.7863 - val_loss: 1.5446 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.3751 - accuracy: 0.8779 - val_loss: 1.7761 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5225 - accuracy: 0.7786 - val_loss: 1.6297 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4300 - accuracy: 0.8550 - val_loss: 1.8962 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4745 - accuracy: 0.8397 - val_loss: 1.6300 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4007 - accuracy: 0.8626 - val_loss: 2.1962 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.4022 - accuracy: 0.8550 - val_loss: 1.4040 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3776 - accuracy: 0.8473 - val_loss: 2.0388 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.3735 - accuracy: 0.8779 - val_loss: 1.8948 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.3722 - accuracy: 0.8550 - val_loss: 1.8228 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.3109 - accuracy: 0.9084 - val_loss: 1.5117 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.3343 - accuracy: 0.8931 - val_loss: 1.5940 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4086 - accuracy: 0.7939 - val_loss: 2.6860 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5401 - accuracy: 0.8168 - val_loss: 1.9936 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4461 - accuracy: 0.7939 - val_loss: 1.8049 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3641 - accuracy: 0.8931 - val_loss: 1.9787 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4150 - accuracy: 0.8397 - val_loss: 4.7823 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.4429 - accuracy: 0.8397 - val_loss: 1.9179 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4712 - accuracy: 0.8321 - val_loss: 2.4302 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4077 - accuracy: 0.8702 - val_loss: 2.2445 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4618 - accuracy: 0.8244 - val_loss: 1.8089 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3525 - accuracy: 0.9008 - val_loss: 2.0629 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3972 - accuracy: 0.8321 - val_loss: 2.5025 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4197 - accuracy: 0.8473 - val_loss: 1.9373 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3612 - accuracy: 0.8626 - val_loss: 1.5524 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3864 - accuracy: 0.8473 - val_loss: 1.6769 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3147 - accuracy: 0.8855 - val_loss: 2.1308 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.3384 - accuracy: 0.8855 - val_loss: 1.9747 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3469 - accuracy: 0.8779 - val_loss: 1.7800 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3974 - accuracy: 0.8626 - val_loss: 1.9790 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3640 - accuracy: 0.8702 - val_loss: 1.9989 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.2953 - accuracy: 0.9237 - val_loss: 2.1506 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.2894 - accuracy: 0.8626 - val_loss: 1.9407 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.2868 - accuracy: 0.9008 - val_loss: 2.1657 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3242 - accuracy: 0.8855 - val_loss: 1.8425 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.2456 - accuracy: 0.9237 - val_loss: 1.4495 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2908 - accuracy: 0.9389 - val_loss: 1.6446 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3494 - accuracy: 0.8550 - val_loss: 2.1204 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2846 - accuracy: 0.9389 - val_loss: 1.6546 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4592 - accuracy: 0.8244 - val_loss: 1.7487 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3537 - accuracy: 0.8702 - val_loss: 2.4019 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2967 - accuracy: 0.8931 - val_loss: 2.8774 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3186 - accuracy: 0.8550 - val_loss: 2.6787 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3716 - accuracy: 0.8550 - val_loss: 2.1798 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3842 - accuracy: 0.8855 - val_loss: 2.3402 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.3596 - accuracy: 0.8473 - val_loss: 2.3366 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3254 - accuracy: 0.8855 - val_loss: 2.0721 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3422 - accuracy: 0.8779 - val_loss: 2.0004 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2838 - accuracy: 0.8779 - val_loss: 2.4170 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2956 - accuracy: 0.8779 - val_loss: 2.1512 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2934 - accuracy: 0.9313 - val_loss: 1.9179 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2958 - accuracy: 0.9160 - val_loss: 2.0043 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.2297 - accuracy: 0.9542 - val_loss: 3.0841 - val_accuracy: 0.2121 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2397 - accuracy: 0.9237 - val_loss: 2.2059 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2976 - accuracy: 0.9008 - val_loss: 3.0199 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3958 - accuracy: 0.8779 - val_loss: 1.8183 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.2009 - accuracy: 0.9542 - val_loss: 1.8549 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3074 - accuracy: 0.8855 - val_loss: 2.2319 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2923 - accuracy: 0.8779 - val_loss: 2.4894 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2788 - accuracy: 0.9084 - val_loss: 2.0637 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2946 - accuracy: 0.9008 - val_loss: 1.9456 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2191 - accuracy: 0.9313 - val_loss: 2.3282 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2796 - accuracy: 0.9237 - val_loss: 1.8592 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.1921 - accuracy: 0.9389 - val_loss: 1.6925 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.1792 - accuracy: 0.9466 - val_loss: 1.7997 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2794 - accuracy: 0.8931 - val_loss: 2.2047 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2300 - accuracy: 0.9389 - val_loss: 2.1643 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2450 - accuracy: 0.8931 - val_loss: 2.2197 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2008 - accuracy: 0.9389 - val_loss: 2.1559 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2275 - accuracy: 0.9466 - val_loss: 2.2127 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4278 - accuracy: 0.8626 - val_loss: 1.9888 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2800 - accuracy: 0.8931 - val_loss: 3.5225 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2670 - accuracy: 0.9160 - val_loss: 1.8965 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.1792 - accuracy: 0.9466 - val_loss: 2.4893 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2670 - accuracy: 0.9237 - val_loss: 2.2334 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.1639 - accuracy: 0.9771 - val_loss: 1.9196 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.1525 - accuracy: 0.9695 - val_loss: 1.9649 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2453 - accuracy: 0.9008 - val_loss: 2.1995 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2408 - accuracy: 0.9466 - val_loss: 2.6943 - val_accuracy: 0.3030 - lr: 0.0010\n",
      "2/2 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 08:58:26 INFO [root] [fit_splits:50] - fold 4: x_train_fold.shape=(131, 1088, 2), y_train_fold.shape=(131, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data4/gsprivate/dl-4-tsc/results/resnet/MTS/1-/fold_4/ exist\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1088, 2)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 1088, 64)             1088      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1088, 64)             256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 1088, 64)             0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 1088, 64)             20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1088, 64)             256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 1088, 64)             0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 1088, 64)             192       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 1088, 64)             12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 1088, 64)             256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 1088, 64)             256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1088, 64)             0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 1088, 64)             0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 1088, 128)            65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1088, 128)            512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 1088, 128)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 1088, 128)            82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1088, 128)            512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 1088, 128)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 1088, 128)            8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 1088, 128)            49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 1088, 128)            512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 1088, 128)            512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1088, 128)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 1088, 128)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 1088, 128)            131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 1088, 128)            512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 1088, 128)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 1088, 128)            82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 1088, 128)            512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 1088, 128)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 1088, 128)            49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 1088, 128)            512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1088, 128)            512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1088, 128)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 1088, 128)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 3)                    387       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 507523 (1.94 MB)\n",
      "Trainable params: 504963 (1.93 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2323 - accuracy: 0.5267"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data4/conda_envs/g2/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 12s 116ms/step - loss: 1.2323 - accuracy: 0.5267 - val_loss: 1.2139 - val_accuracy: 0.1515 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 1.0512 - accuracy: 0.4962 - val_loss: 1.2770 - val_accuracy: 0.2424 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.9551 - accuracy: 0.6336 - val_loss: 1.1031 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8736 - accuracy: 0.6412 - val_loss: 1.0637 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8759 - accuracy: 0.6107 - val_loss: 1.0384 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 0.8817 - accuracy: 0.6260 - val_loss: 1.1401 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8809 - accuracy: 0.5954 - val_loss: 1.1187 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.9251 - accuracy: 0.6031 - val_loss: 1.0630 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.8155 - accuracy: 0.6260 - val_loss: 1.0260 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8666 - accuracy: 0.6183 - val_loss: 1.0484 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.8070 - accuracy: 0.6489 - val_loss: 1.0708 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8699 - accuracy: 0.5954 - val_loss: 1.0878 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8142 - accuracy: 0.6489 - val_loss: 1.1074 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8306 - accuracy: 0.6260 - val_loss: 1.0977 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8210 - accuracy: 0.6260 - val_loss: 1.0606 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8560 - accuracy: 0.6412 - val_loss: 1.0944 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8216 - accuracy: 0.6107 - val_loss: 1.1223 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.7938 - accuracy: 0.6641 - val_loss: 1.0761 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7961 - accuracy: 0.6412 - val_loss: 1.0914 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8069 - accuracy: 0.6412 - val_loss: 1.1083 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7917 - accuracy: 0.6489 - val_loss: 1.1185 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.7640 - accuracy: 0.6031 - val_loss: 1.1372 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8082 - accuracy: 0.6718 - val_loss: 1.1544 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7799 - accuracy: 0.6260 - val_loss: 1.1258 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.8188 - accuracy: 0.6260 - val_loss: 1.0956 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7757 - accuracy: 0.6718 - val_loss: 1.1464 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7955 - accuracy: 0.6641 - val_loss: 1.1846 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.7545 - accuracy: 0.6336 - val_loss: 1.1098 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7719 - accuracy: 0.6107 - val_loss: 1.1200 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7827 - accuracy: 0.6489 - val_loss: 1.0959 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.7437 - accuracy: 0.6260 - val_loss: 1.0793 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8165 - accuracy: 0.5878 - val_loss: 1.1677 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.8093 - accuracy: 0.6107 - val_loss: 1.1680 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7732 - accuracy: 0.6947 - val_loss: 1.1186 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7553 - accuracy: 0.6489 - val_loss: 1.0858 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7565 - accuracy: 0.6794 - val_loss: 1.2931 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7602 - accuracy: 0.6565 - val_loss: 1.4208 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7730 - accuracy: 0.6412 - val_loss: 1.2050 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7391 - accuracy: 0.6718 - val_loss: 1.3389 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.7282 - accuracy: 0.7099 - val_loss: 1.1687 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7168 - accuracy: 0.6794 - val_loss: 1.0799 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 47ms/step - loss: 0.6923 - accuracy: 0.6947 - val_loss: 1.1285 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7240 - accuracy: 0.6794 - val_loss: 1.1332 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8812 - accuracy: 0.6031 - val_loss: 1.2371 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7387 - accuracy: 0.6565 - val_loss: 1.4227 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7190 - accuracy: 0.6794 - val_loss: 1.3047 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8062 - accuracy: 0.6565 - val_loss: 1.1845 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7568 - accuracy: 0.6260 - val_loss: 1.0884 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7270 - accuracy: 0.6794 - val_loss: 1.1378 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7262 - accuracy: 0.6641 - val_loss: 1.1115 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7069 - accuracy: 0.6870 - val_loss: 1.1322 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7085 - accuracy: 0.6565 - val_loss: 1.4343 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7478 - accuracy: 0.6183 - val_loss: 1.2330 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7240 - accuracy: 0.6565 - val_loss: 1.2879 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7357 - accuracy: 0.6565 - val_loss: 1.2699 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7620 - accuracy: 0.6489 - val_loss: 1.4320 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.6985 - accuracy: 0.7023 - val_loss: 1.7285 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 0.7719 - accuracy: 0.6489 - val_loss: 1.3821 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.7148 - accuracy: 0.6489 - val_loss: 1.1165 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 1s 48ms/step - loss: 0.7227 - accuracy: 0.6794 - val_loss: 1.4492 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7110 - accuracy: 0.7099 - val_loss: 1.3684 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 1s 50ms/step - loss: 0.6878 - accuracy: 0.6718 - val_loss: 1.2632 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.6745 - accuracy: 0.7405 - val_loss: 2.5880 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7518 - accuracy: 0.6412 - val_loss: 1.4412 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.6266 - accuracy: 0.7405 - val_loss: 1.3699 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6438 - accuracy: 0.7328 - val_loss: 1.2108 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6993 - accuracy: 0.7481 - val_loss: 1.3501 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6426 - accuracy: 0.7252 - val_loss: 1.2149 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7108 - accuracy: 0.6794 - val_loss: 1.6651 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7265 - accuracy: 0.6718 - val_loss: 1.7334 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7253 - accuracy: 0.6718 - val_loss: 2.1032 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6879 - accuracy: 0.6718 - val_loss: 1.6645 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7135 - accuracy: 0.7405 - val_loss: 1.4294 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.6185 - accuracy: 0.7405 - val_loss: 1.4018 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.6020 - accuracy: 0.7405 - val_loss: 1.4795 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6615 - accuracy: 0.6870 - val_loss: 2.2024 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6333 - accuracy: 0.7099 - val_loss: 1.9360 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6513 - accuracy: 0.7328 - val_loss: 1.3372 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5757 - accuracy: 0.7252 - val_loss: 1.5072 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5875 - accuracy: 0.7710 - val_loss: 1.4821 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6334 - accuracy: 0.7328 - val_loss: 2.4641 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7054 - accuracy: 0.7405 - val_loss: 2.4965 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.6574 - accuracy: 0.6794 - val_loss: 1.6643 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6571 - accuracy: 0.7557 - val_loss: 1.1998 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5641 - accuracy: 0.7405 - val_loss: 1.3685 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5565 - accuracy: 0.7328 - val_loss: 1.1859 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.5179 - accuracy: 0.7557 - val_loss: 1.1821 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5524 - accuracy: 0.7405 - val_loss: 1.1249 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5331 - accuracy: 0.7328 - val_loss: 1.4334 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6618 - accuracy: 0.7023 - val_loss: 1.6546 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5592 - accuracy: 0.7557 - val_loss: 1.7068 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5448 - accuracy: 0.7557 - val_loss: 2.1340 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6840 - accuracy: 0.7710 - val_loss: 1.9671 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6292 - accuracy: 0.7481 - val_loss: 1.9682 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6102 - accuracy: 0.7176 - val_loss: 1.7626 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5923 - accuracy: 0.7634 - val_loss: 1.3251 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5247 - accuracy: 0.7863 - val_loss: 1.2300 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5384 - accuracy: 0.7634 - val_loss: 1.5598 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5914 - accuracy: 0.7405 - val_loss: 2.5387 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8087 - accuracy: 0.6794 - val_loss: 3.9589 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6001 - accuracy: 0.7405 - val_loss: 3.7560 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5720 - accuracy: 0.7710 - val_loss: 2.1555 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5767 - accuracy: 0.7405 - val_loss: 2.5754 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5294 - accuracy: 0.7939 - val_loss: 1.7143 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.5291 - accuracy: 0.7710 - val_loss: 1.3968 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.4863 - accuracy: 0.7863 - val_loss: 1.3312 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5538 - accuracy: 0.7557 - val_loss: 1.5835 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5443 - accuracy: 0.8244 - val_loss: 1.3469 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5089 - accuracy: 0.8092 - val_loss: 1.6494 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5185 - accuracy: 0.8168 - val_loss: 1.4086 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5086 - accuracy: 0.8015 - val_loss: 1.5604 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5812 - accuracy: 0.7557 - val_loss: 1.4438 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4946 - accuracy: 0.7557 - val_loss: 1.5796 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4946 - accuracy: 0.7939 - val_loss: 1.3709 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.4460 - accuracy: 0.8092 - val_loss: 1.4898 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4835 - accuracy: 0.7786 - val_loss: 1.6079 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5482 - accuracy: 0.7710 - val_loss: 1.3469 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4580 - accuracy: 0.8244 - val_loss: 1.5775 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5086 - accuracy: 0.7863 - val_loss: 1.2811 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5332 - accuracy: 0.8244 - val_loss: 1.3328 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4654 - accuracy: 0.8473 - val_loss: 1.4940 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4633 - accuracy: 0.8397 - val_loss: 1.5449 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4620 - accuracy: 0.8015 - val_loss: 1.3584 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.4264 - accuracy: 0.8550 - val_loss: 1.5063 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.3415 - accuracy: 0.8931 - val_loss: 1.3428 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4387 - accuracy: 0.8321 - val_loss: 1.5423 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4503 - accuracy: 0.8168 - val_loss: 1.6274 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3538 - accuracy: 0.8931 - val_loss: 1.8402 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4162 - accuracy: 0.8321 - val_loss: 1.2848 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4452 - accuracy: 0.8244 - val_loss: 1.4318 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5025 - accuracy: 0.7710 - val_loss: 1.5860 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4277 - accuracy: 0.8473 - val_loss: 2.1893 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4428 - accuracy: 0.8397 - val_loss: 1.7119 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 0.4125 - accuracy: 0.8626 - val_loss: 2.2022 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3795 - accuracy: 0.8550 - val_loss: 1.4400 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3706 - accuracy: 0.8702 - val_loss: 1.4322 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3698 - accuracy: 0.8779 - val_loss: 1.6135 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.2818 - accuracy: 0.9008 - val_loss: 1.5046 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3417 - accuracy: 0.9084 - val_loss: 1.4975 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.2676 - accuracy: 0.9008 - val_loss: 1.5913 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.3666 - accuracy: 0.8473 - val_loss: 1.6397 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3352 - accuracy: 0.8855 - val_loss: 1.4611 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3266 - accuracy: 0.9389 - val_loss: 2.1233 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.3861 - accuracy: 0.8550 - val_loss: 1.4888 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4094 - accuracy: 0.8397 - val_loss: 1.2714 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2760 - accuracy: 0.9237 - val_loss: 1.4956 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3767 - accuracy: 0.8473 - val_loss: 1.7246 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3351 - accuracy: 0.8931 - val_loss: 2.5714 - val_accuracy: 0.3939 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2867 - accuracy: 0.9084 - val_loss: 2.2277 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2774 - accuracy: 0.9389 - val_loss: 1.2057 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2741 - accuracy: 0.9389 - val_loss: 1.4219 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3135 - accuracy: 0.8855 - val_loss: 1.3446 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3551 - accuracy: 0.8779 - val_loss: 1.5323 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2718 - accuracy: 0.9389 - val_loss: 2.3117 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2827 - accuracy: 0.9084 - val_loss: 1.9500 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.2618 - accuracy: 0.8931 - val_loss: 1.7581 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.2328 - accuracy: 0.9313 - val_loss: 1.7430 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.2193 - accuracy: 0.9542 - val_loss: 1.9820 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3098 - accuracy: 0.9160 - val_loss: 1.6955 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2735 - accuracy: 0.9008 - val_loss: 1.4658 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2452 - accuracy: 0.9237 - val_loss: 1.8770 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2989 - accuracy: 0.9008 - val_loss: 1.9728 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2243 - accuracy: 0.9313 - val_loss: 1.5952 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.2125 - accuracy: 0.9313 - val_loss: 1.6158 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.2293 - accuracy: 0.9313 - val_loss: 2.2942 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2734 - accuracy: 0.9084 - val_loss: 1.7426 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.2580 - accuracy: 0.9084 - val_loss: 1.8558 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.2308 - accuracy: 0.9160 - val_loss: 1.8439 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.2770 - accuracy: 0.9160 - val_loss: 2.3037 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.3504 - accuracy: 0.8779 - val_loss: 4.0341 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.4029 - accuracy: 0.8473 - val_loss: 2.9396 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3379 - accuracy: 0.8931 - val_loss: 2.1552 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2584 - accuracy: 0.9389 - val_loss: 1.5400 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3489 - accuracy: 0.8397 - val_loss: 2.3801 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2565 - accuracy: 0.9389 - val_loss: 1.8157 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2748 - accuracy: 0.9008 - val_loss: 4.2108 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2685 - accuracy: 0.9160 - val_loss: 1.6600 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2581 - accuracy: 0.9237 - val_loss: 1.5650 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.1733 - accuracy: 0.9466 - val_loss: 1.6978 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.1595 - accuracy: 0.9618 - val_loss: 1.6844 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2129 - accuracy: 0.9160 - val_loss: 1.6132 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2500 - accuracy: 0.9237 - val_loss: 1.6356 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.2146 - accuracy: 0.9160 - val_loss: 1.5748 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2627 - accuracy: 0.9389 - val_loss: 1.9559 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2254 - accuracy: 0.9389 - val_loss: 1.8327 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2002 - accuracy: 0.9466 - val_loss: 1.9761 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2693 - accuracy: 0.8626 - val_loss: 2.3639 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.2063 - accuracy: 0.9618 - val_loss: 2.0903 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2410 - accuracy: 0.9008 - val_loss: 1.7630 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.1515 - accuracy: 0.9466 - val_loss: 2.9667 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1915 - accuracy: 0.9389 - val_loss: 2.8503 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2186 - accuracy: 0.9389 - val_loss: 2.5486 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.2359 - accuracy: 0.9237 - val_loss: 3.3154 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.2254 - accuracy: 0.9160 - val_loss: 2.0581 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1797 - accuracy: 0.9695 - val_loss: 2.3253 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2083 - accuracy: 0.9160 - val_loss: 2.0625 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1780 - accuracy: 0.9466 - val_loss: 1.9738 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1788 - accuracy: 0.9389 - val_loss: 1.8982 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.1317 - accuracy: 0.9771 - val_loss: 1.8979 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.1546 - accuracy: 0.9695 - val_loss: 1.8036 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "2/2 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 09:00:03 INFO [root] [fit_splits:50] - fold 5: x_train_fold.shape=(132, 1088, 2), y_train_fold.shape=(132, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data4/gsprivate/dl-4-tsc/results/resnet/MTS/1-/fold_5/ exist\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1088, 2)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 1088, 64)             1088      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1088, 64)             256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 1088, 64)             0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 1088, 64)             20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1088, 64)             256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 1088, 64)             0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 1088, 64)             192       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 1088, 64)             12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 1088, 64)             256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 1088, 64)             256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1088, 64)             0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 1088, 64)             0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 1088, 128)            65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1088, 128)            512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 1088, 128)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 1088, 128)            82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1088, 128)            512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 1088, 128)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 1088, 128)            8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 1088, 128)            49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 1088, 128)            512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 1088, 128)            512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1088, 128)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 1088, 128)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 1088, 128)            131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 1088, 128)            512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 1088, 128)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 1088, 128)            82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 1088, 128)            512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 1088, 128)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 1088, 128)            49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 1088, 128)            512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1088, 128)            512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1088, 128)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 1088, 128)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 3)                    387       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 507523 (1.94 MB)\n",
      "Trainable params: 504963 (1.93 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - 12s 145ms/step - loss: 1.3389 - accuracy: 0.5000 - val_loss: 1.2049 - val_accuracy: 0.1562 - lr: 0.0010\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data4/conda_envs/g2/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 54ms/step - loss: 1.0966 - accuracy: 0.5303 - val_loss: 1.3202 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 1.0055 - accuracy: 0.5985 - val_loss: 1.3022 - val_accuracy: 0.2812 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.9425 - accuracy: 0.5833 - val_loss: 1.1412 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.9857 - accuracy: 0.5758 - val_loss: 0.9594 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.9209 - accuracy: 0.5682 - val_loss: 0.9468 - val_accuracy: 0.5938 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.8843 - accuracy: 0.5985 - val_loss: 0.9743 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8621 - accuracy: 0.5909 - val_loss: 0.9677 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8698 - accuracy: 0.5530 - val_loss: 0.9241 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8864 - accuracy: 0.5682 - val_loss: 0.9339 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.8472 - accuracy: 0.6364 - val_loss: 0.9503 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8763 - accuracy: 0.5985 - val_loss: 0.9676 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8918 - accuracy: 0.5606 - val_loss: 1.0011 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.8449 - accuracy: 0.6288 - val_loss: 1.0438 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8554 - accuracy: 0.6136 - val_loss: 1.0177 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8129 - accuracy: 0.6212 - val_loss: 0.9485 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8382 - accuracy: 0.6061 - val_loss: 0.9644 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8049 - accuracy: 0.6212 - val_loss: 0.9730 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8133 - accuracy: 0.6364 - val_loss: 0.9515 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8079 - accuracy: 0.6288 - val_loss: 0.8857 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8341 - accuracy: 0.5833 - val_loss: 0.8530 - val_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8467 - accuracy: 0.5909 - val_loss: 0.8247 - val_accuracy: 0.5938 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8063 - accuracy: 0.6364 - val_loss: 0.8749 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.8601 - accuracy: 0.6136 - val_loss: 0.9325 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8740 - accuracy: 0.5909 - val_loss: 0.9941 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8506 - accuracy: 0.6212 - val_loss: 1.0109 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8339 - accuracy: 0.6439 - val_loss: 0.9269 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8180 - accuracy: 0.6667 - val_loss: 0.9652 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8562 - accuracy: 0.6136 - val_loss: 1.0138 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.8124 - accuracy: 0.6288 - val_loss: 1.0239 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8254 - accuracy: 0.6212 - val_loss: 0.9084 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8138 - accuracy: 0.6667 - val_loss: 0.9434 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8251 - accuracy: 0.6136 - val_loss: 0.9852 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8307 - accuracy: 0.6288 - val_loss: 1.0461 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.7910 - accuracy: 0.6439 - val_loss: 1.0652 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.7828 - accuracy: 0.5985 - val_loss: 1.0429 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7814 - accuracy: 0.6288 - val_loss: 1.0850 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7901 - accuracy: 0.6212 - val_loss: 1.0663 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7818 - accuracy: 0.6667 - val_loss: 1.0122 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7661 - accuracy: 0.6515 - val_loss: 1.0066 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.8324 - accuracy: 0.6061 - val_loss: 1.0370 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.7551 - accuracy: 0.6894 - val_loss: 1.1047 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7843 - accuracy: 0.6212 - val_loss: 1.0184 - val_accuracy: 0.5938 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7780 - accuracy: 0.6515 - val_loss: 1.0301 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8719 - accuracy: 0.6288 - val_loss: 1.4349 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8230 - accuracy: 0.6364 - val_loss: 2.1554 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7980 - accuracy: 0.6591 - val_loss: 2.2800 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.7355 - accuracy: 0.6364 - val_loss: 1.5904 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7729 - accuracy: 0.6667 - val_loss: 1.2915 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7466 - accuracy: 0.6667 - val_loss: 1.3891 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7235 - accuracy: 0.7121 - val_loss: 1.2699 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7121 - accuracy: 0.6667 - val_loss: 1.1304 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.6758 - accuracy: 0.7348 - val_loss: 1.1813 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7921 - accuracy: 0.6439 - val_loss: 1.2778 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7438 - accuracy: 0.6742 - val_loss: 1.6583 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7508 - accuracy: 0.7273 - val_loss: 1.6881 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7554 - accuracy: 0.6591 - val_loss: 2.1582 - val_accuracy: 0.2812 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8050 - accuracy: 0.6591 - val_loss: 3.1555 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.8344 - accuracy: 0.6364 - val_loss: 2.7641 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.8306 - accuracy: 0.6288 - val_loss: 1.4662 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7355 - accuracy: 0.6970 - val_loss: 1.3165 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7487 - accuracy: 0.6818 - val_loss: 1.3666 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7137 - accuracy: 0.6515 - val_loss: 1.3186 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7678 - accuracy: 0.6591 - val_loss: 1.2960 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.6464 - accuracy: 0.7424 - val_loss: 1.2245 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6933 - accuracy: 0.7273 - val_loss: 1.1543 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7049 - accuracy: 0.7273 - val_loss: 1.1692 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7293 - accuracy: 0.7348 - val_loss: 1.8509 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7549 - accuracy: 0.6591 - val_loss: 1.8046 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.7536 - accuracy: 0.6364 - val_loss: 1.3732 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7697 - accuracy: 0.6970 - val_loss: 1.3046 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6977 - accuracy: 0.6818 - val_loss: 1.2116 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6637 - accuracy: 0.7197 - val_loss: 1.2173 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6702 - accuracy: 0.7424 - val_loss: 1.3756 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6566 - accuracy: 0.7500 - val_loss: 2.3049 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.6346 - accuracy: 0.7500 - val_loss: 1.0953 - val_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.6826 - accuracy: 0.7121 - val_loss: 1.3153 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7208 - accuracy: 0.6742 - val_loss: 1.4047 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.8140 - accuracy: 0.6212 - val_loss: 2.0785 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.7187 - accuracy: 0.6818 - val_loss: 1.3883 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6706 - accuracy: 0.7197 - val_loss: 1.1167 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6677 - accuracy: 0.7197 - val_loss: 1.2761 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6808 - accuracy: 0.6970 - val_loss: 1.1531 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.6325 - accuracy: 0.7348 - val_loss: 1.1629 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6657 - accuracy: 0.7197 - val_loss: 1.1830 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.5978 - accuracy: 0.7348 - val_loss: 1.1316 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6470 - accuracy: 0.6970 - val_loss: 1.8364 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6553 - accuracy: 0.7197 - val_loss: 1.3613 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6527 - accuracy: 0.7045 - val_loss: 1.3023 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.6408 - accuracy: 0.7197 - val_loss: 1.3308 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7630 - accuracy: 0.6894 - val_loss: 1.9069 - val_accuracy: 0.2812 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6932 - accuracy: 0.7121 - val_loss: 1.4498 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6328 - accuracy: 0.6970 - val_loss: 1.2764 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6000 - accuracy: 0.7197 - val_loss: 1.3298 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5849 - accuracy: 0.7576 - val_loss: 1.1796 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 0.5706 - accuracy: 0.7652 - val_loss: 1.1221 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5212 - accuracy: 0.7879 - val_loss: 1.4616 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5573 - accuracy: 0.7424 - val_loss: 1.1654 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6629 - accuracy: 0.7045 - val_loss: 1.7465 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6666 - accuracy: 0.6894 - val_loss: 1.8772 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.6139 - accuracy: 0.7197 - val_loss: 1.1017 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5601 - accuracy: 0.7576 - val_loss: 1.0578 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6751 - accuracy: 0.7197 - val_loss: 1.6360 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7215 - accuracy: 0.6818 - val_loss: 5.4532 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.7892 - accuracy: 0.6667 - val_loss: 2.3793 - val_accuracy: 0.2812 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6848 - accuracy: 0.6970 - val_loss: 1.3964 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6682 - accuracy: 0.6818 - val_loss: 1.0731 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6415 - accuracy: 0.7576 - val_loss: 1.1194 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6286 - accuracy: 0.7576 - val_loss: 1.4886 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6359 - accuracy: 0.7652 - val_loss: 1.3165 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.5592 - accuracy: 0.7955 - val_loss: 1.5591 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6223 - accuracy: 0.7197 - val_loss: 1.6231 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6053 - accuracy: 0.7803 - val_loss: 1.4474 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6624 - accuracy: 0.6894 - val_loss: 1.3841 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5824 - accuracy: 0.7652 - val_loss: 1.3073 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5784 - accuracy: 0.7652 - val_loss: 1.0830 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5543 - accuracy: 0.7424 - val_loss: 1.1362 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5358 - accuracy: 0.7576 - val_loss: 1.4497 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6056 - accuracy: 0.7273 - val_loss: 1.3816 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5511 - accuracy: 0.7955 - val_loss: 1.8114 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6168 - accuracy: 0.7576 - val_loss: 1.2813 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5309 - accuracy: 0.8030 - val_loss: 1.2443 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5626 - accuracy: 0.7727 - val_loss: 1.0372 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5338 - accuracy: 0.7879 - val_loss: 1.2024 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.6547 - accuracy: 0.6970 - val_loss: 1.2859 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5893 - accuracy: 0.7424 - val_loss: 2.7642 - val_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6221 - accuracy: 0.7348 - val_loss: 3.5627 - val_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5312 - accuracy: 0.7803 - val_loss: 1.3776 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5488 - accuracy: 0.7424 - val_loss: 1.9671 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5638 - accuracy: 0.7652 - val_loss: 4.9599 - val_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6060 - accuracy: 0.7576 - val_loss: 2.9391 - val_accuracy: 0.2812 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.5358 - accuracy: 0.7652 - val_loss: 1.3600 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.5028 - accuracy: 0.8030 - val_loss: 1.1906 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5406 - accuracy: 0.7576 - val_loss: 1.2288 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6112 - accuracy: 0.7348 - val_loss: 1.9842 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5104 - accuracy: 0.8106 - val_loss: 1.8500 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5425 - accuracy: 0.7879 - val_loss: 1.7705 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5169 - accuracy: 0.8106 - val_loss: 1.2181 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.4910 - accuracy: 0.8182 - val_loss: 1.2024 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5468 - accuracy: 0.8030 - val_loss: 1.3749 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5222 - accuracy: 0.7803 - val_loss: 1.1595 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.6736 - accuracy: 0.7803 - val_loss: 2.1453 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5092 - accuracy: 0.8182 - val_loss: 1.9968 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.4775 - accuracy: 0.8106 - val_loss: 2.2389 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5116 - accuracy: 0.7803 - val_loss: 1.7425 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.4285 - accuracy: 0.8106 - val_loss: 1.7721 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5107 - accuracy: 0.7879 - val_loss: 2.4323 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5795 - accuracy: 0.7576 - val_loss: 1.3044 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5141 - accuracy: 0.7803 - val_loss: 1.1603 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4837 - accuracy: 0.7879 - val_loss: 0.8218 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4918 - accuracy: 0.7955 - val_loss: 1.1262 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4742 - accuracy: 0.8333 - val_loss: 1.1185 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4990 - accuracy: 0.7955 - val_loss: 1.4566 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4812 - accuracy: 0.8106 - val_loss: 1.3214 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.5108 - accuracy: 0.7803 - val_loss: 1.9083 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5544 - accuracy: 0.7879 - val_loss: 1.9813 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4382 - accuracy: 0.8333 - val_loss: 1.4710 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4809 - accuracy: 0.7879 - val_loss: 1.8617 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.3995 - accuracy: 0.8788 - val_loss: 1.6737 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4218 - accuracy: 0.8106 - val_loss: 1.5100 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.3884 - accuracy: 0.8788 - val_loss: 1.5860 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.5309 - accuracy: 0.7576 - val_loss: 1.5580 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4511 - accuracy: 0.8409 - val_loss: 1.5151 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5085 - accuracy: 0.8030 - val_loss: 3.2412 - val_accuracy: 0.2812 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4851 - accuracy: 0.8333 - val_loss: 3.9025 - val_accuracy: 0.1250 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5076 - accuracy: 0.7879 - val_loss: 3.0487 - val_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.5062 - accuracy: 0.8030 - val_loss: 2.1372 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4454 - accuracy: 0.7955 - val_loss: 1.6091 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4290 - accuracy: 0.8636 - val_loss: 1.3441 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.3939 - accuracy: 0.8409 - val_loss: 1.3364 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4448 - accuracy: 0.8182 - val_loss: 1.4922 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 1s 57ms/step - loss: 0.3562 - accuracy: 0.8788 - val_loss: 1.4577 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4497 - accuracy: 0.7955 - val_loss: 1.1023 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4947 - accuracy: 0.7803 - val_loss: 1.3926 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4806 - accuracy: 0.8106 - val_loss: 1.2129 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4748 - accuracy: 0.8182 - val_loss: 2.8633 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.6094 - accuracy: 0.7652 - val_loss: 2.2790 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.5030 - accuracy: 0.7955 - val_loss: 1.8776 - val_accuracy: 0.2812 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.4864 - accuracy: 0.8182 - val_loss: 1.9585 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4579 - accuracy: 0.8258 - val_loss: 1.1193 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.3572 - accuracy: 0.8788 - val_loss: 1.3536 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.3551 - accuracy: 0.8712 - val_loss: 1.5235 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3557 - accuracy: 0.8939 - val_loss: 2.2878 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4549 - accuracy: 0.8182 - val_loss: 2.5728 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.4559 - accuracy: 0.7955 - val_loss: 1.8327 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4111 - accuracy: 0.8106 - val_loss: 1.6405 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4059 - accuracy: 0.8182 - val_loss: 1.3382 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.4416 - accuracy: 0.8106 - val_loss: 2.3420 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.4923 - accuracy: 0.8182 - val_loss: 1.5267 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3805 - accuracy: 0.8712 - val_loss: 1.7421 - val_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.3345 - accuracy: 0.8712 - val_loss: 1.4920 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.3012 - accuracy: 0.8939 - val_loss: 1.2925 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3644 - accuracy: 0.8333 - val_loss: 1.4087 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3232 - accuracy: 0.9015 - val_loss: 1.5330 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.2838 - accuracy: 0.9091 - val_loss: 1.5243 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3052 - accuracy: 0.9015 - val_loss: 1.5651 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.2941 - accuracy: 0.9167 - val_loss: 1.7702 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.2952 - accuracy: 0.9015 - val_loss: 2.0755 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.4696 - accuracy: 0.7652 - val_loss: 2.0748 - val_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3866 - accuracy: 0.8409 - val_loss: 1.8473 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0df0511550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 09:01:38 WARNING [tensorflow] [called_with_tracing:159] - 5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0df0511550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 449ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11970/1481462799.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgeng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DONE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data4/gsprivate/dl-4-tsc/utils/geng.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(classifier, X, Y, batch_size, epochs, n_splits, learning_rate)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mfold_no\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0msummarize_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistories\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# This function remains the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data4/gsprivate/dl-4-tsc/utils/geng.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(histories)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# 遍历所有历史记录\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# 计算每一折的平均损失和准确率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mtotal_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mtotal_val_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mtotal_val_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data4/conda_envs/g2/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'history'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "logconfig.setup_logging(dir=output_directory)\n",
    "geng.fit_splits(resnet_classifier,x,y,batch_size=16,epochs=200)\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "import tensorflow as tf\n",
    "\n",
    "# print(torch.version.cuda)·\n",
    "# print(torch.backends.cudnn.version())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Retrieving all environment variables\n",
    "# env_vars = os.environ\n",
    "\n",
    "# # Displaying the environment variables\n",
    "# env_vars_dict = {key: env_vars[key] for key in env_vars}\n",
    "# env_vars_dict['LD_LIBRARY_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_name=\"fcn\"\n",
    "# root_dir=\"/data4/gsprivate/dl-4-tsc\"\n",
    "# output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '' + '/' + \\\n",
    "#                     dataset_name + '/'\n",
    "                    \n",
    "# if create_directory(output_directory) is None:\n",
    "#     print(\"Creating directory:{} None\".format(output_directory))\n",
    "\n",
    "# x,y,input_shape=data_final_process(final_data,labels)\n",
    "# fcn_classifier = geng.create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
    "# print(f\"Classifier x.shape={x.shape} ,y.shape={y.shape}, input_shape={input_shape}, nb_classes={nb_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_name=\"cnn\"\n",
    "# root_dir=\"/data4/gsprivate/dl-4-tsc\"\n",
    "# output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '' + '/' + \\\n",
    "#                     dataset_name + '/'\n",
    "                    \n",
    "# if create_directory(output_directory) is None:\n",
    "#     print(\"Creating directory:{} None\".format(output_directory))\n",
    "\n",
    "# x,y,input_shape=data_final_process(final_data,labels)\n",
    "# cnn_classifier = geng.create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
    "# print(f\"Classifier x.shape={x.shape} ,y.shape={y.shape}, input_shape={input_shape}, nb_classes={nb_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_name=\"mcnn\"\n",
    "# root_dir=\"/data4/gsprivate/dl-4-tsc\"\n",
    "# output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '' + '/' + \\\n",
    "#                     dataset_name + '/'\n",
    "                    \n",
    "# if create_directory(output_directory) is None:\n",
    "#     print(\"Creating directory:{} None\".format(output_directory))\n",
    "\n",
    "# x,y,input_shape=data_final_process(final_data,labels)\n",
    "# mcnn_classifier = geng.create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
    "# print(f\"Classifier x.shape={x.shape} ,y.shape={y.shape}, input_shape={input_shape}, nb_classes={nb_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture cap --no-stderr\n",
    "# # print(\"这将写入到文件中\")\n",
    "# logconfig.setup_logging(dir=\"/data4/gsprivate/dl-4-tsc/results/mcnn/MTS/Kailuan\")\n",
    "# geng.fit_splits_for_mcnn(mcnn_classifier,x,y,epochs=250)\n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsprivate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
